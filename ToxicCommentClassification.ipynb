{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ToxicCommentClassification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGuBgYdiz_07",
        "colab_type": "text"
      },
      "source": [
        "# MOUNTING THE DRIVE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgyrku5FDQ8q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "56af7923-83c7-475c-ba70-22a9dbdf4573"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4KhtssJ0HFW",
        "colab_type": "text"
      },
      "source": [
        "# READING THE DATA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eugHSJPwnu_C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "a20e096a-3e75-4678-9171-56b5ec0316fb"
      },
      "source": [
        "import pandas as pd\n",
        "data = pd.read_csv('/content/drive/My Drive/Datasets/ToxicCommentProject/train.csv/train.csv')\n",
        "# The session was crashing and that is whhy I took a very small sample to try the algorithms\n",
        "data = data.sample(frac = 0.01).reset_index(drop = True)\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>comment_text</th>\n",
              "      <th>toxic</th>\n",
              "      <th>severe_toxic</th>\n",
              "      <th>obscene</th>\n",
              "      <th>threat</th>\n",
              "      <th>insult</th>\n",
              "      <th>identity_hate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>bd46839761b63064</td>\n",
              "      <td>I removed the speedy tag on the strength of th...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6774bca4d6dbf97f</td>\n",
              "      <td>Please stop. If you continue to blank out or d...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>02f516a2a8f57506</td>\n",
              "      <td>Effects under £3000 - dated 3rd June (with one...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>925f325f0a056907</td>\n",
              "      <td>\"\\n\\nI don't know which \"\"personal attacks\"\" y...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3a16fababd9bf78e</td>\n",
              "      <td>If you expect everyone here to agree with you ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 id  ... identity_hate\n",
              "0  bd46839761b63064  ...             0\n",
              "1  6774bca4d6dbf97f  ...             0\n",
              "2  02f516a2a8f57506  ...             0\n",
              "3  925f325f0a056907  ...             0\n",
              "4  3a16fababd9bf78e  ...             0\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTYPTnCJr5uk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3f6e80a8-7141-4e7d-83a3-3de24589e5f3"
      },
      "source": [
        "data.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1596, 8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRWKR1P20Yh5",
        "colab_type": "text"
      },
      "source": [
        "# IMPORTING LIBRARIES "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8_M4DFxuEwo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KiZcJTQS0lC-",
        "colab_type": "text"
      },
      "source": [
        "# TEXT PROCESSING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Ew6KPrK0r9h",
        "colab_type": "text"
      },
      "source": [
        "#### Converting upper-case to lower-case"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zACBqr_4eNj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "19f21da9-2187-4449-ae6b-86172a8f1d4f"
      },
      "source": [
        "data['comment_text'] = data['comment_text'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
        "data['comment_text'].head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    i removed the speedy tag on the strength of th...\n",
              "1    please stop. if you continue to blank out or d...\n",
              "2    effects under £3000 - dated 3rd june (with one...\n",
              "3    \" i don't know which \"\"personal attacks\"\" you ...\n",
              "4    if you expect everyone here to agree with you ...\n",
              "Name: comment_text, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyIcuYev05Wo",
        "colab_type": "text"
      },
      "source": [
        "#### Removing stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ln37a7AH5aKC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "fb539bd3-5c8d-401c-8f33-7053bdaa670f"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "stop = stopwords.words('english')\n",
        "data['comment_text'] = data['comment_text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
        "data['comment_text'].head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       removed speedy tag strength times story alone.\n",
              "1    please stop. continue blank delete portions pa...\n",
              "2    effects £3000 - dated 3rd june (with one codic...\n",
              "3    \" know \"\"personal attacks\"\" referring to. sinc...\n",
              "4                expect everyone agree terribly wrong.\n",
              "Name: comment_text, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKQ3utFm0_nm",
        "colab_type": "text"
      },
      "source": [
        "#### Removing the very common words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0B21A1YG5-ru",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "common_words = pd.Series(' '.join(data['comment_text']).split()).value_counts()[:10]\n",
        "common_words = list(common_words.index)\n",
        "data['comment_text'] = data['comment_text'].apply(lambda x: \" \".join(x for x in x.split() if x not in common_words))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yT8WzeEH1HDt",
        "colab_type": "text"
      },
      "source": [
        "#### Removing the rare words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8CYlghJHyLCf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rare_words = pd.Series(' '.join(data['comment_text']).split()).value_counts()[-10:]\n",
        "rare_words = list(rare_words.index)\n",
        "data['comment_text'] = data['comment_text'].apply(lambda x: \" \".join(x for x in x.split() if x not in rare_words))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8d6KWdV2H74",
        "colab_type": "text"
      },
      "source": [
        "#### Removing tags, punctuation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osOgRek0wQfv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tagremoval(text):\n",
        "  comp = re.compile('<.*?>')\n",
        "  cl_text = re.sub(comp, '', text)\n",
        "  return cl_text\n",
        "\n",
        "def Puncremoval(text): #function to clean the word of any punctuation or special characters\n",
        "    cl_text = re.sub(r'[?|!|\\'|\"|#]',r'', text)\n",
        "    cl_text = re.sub(r'[.|,|)|(|\\|/]',r' ',cl_text)\n",
        "    cl_text = cl_text.strip()\n",
        "    cl_text = cl_text.replace(\"\\n\",\" \")\n",
        "    return cl_text\n",
        "\n",
        "def processed(text):\n",
        "    processed_text = \"\"\n",
        "    for word in text.split():\n",
        "        processed_word = re.sub('[^a-z A-Z]+', ' ', word)\n",
        "        processed_text += processed_word\n",
        "        processed_text += \" \"\n",
        "    processed_text = processed_text.strip()\n",
        "    return processed_text\n",
        "\n",
        "\n",
        "data['comment_text'] = data['comment_text'].apply(tagremoval)\n",
        "data['comment_text'] = data['comment_text'].apply(Puncremoval)\n",
        "data['comment_text'] = data['comment_text'].apply(processed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nP0EUwtk2PuC",
        "colab_type": "text"
      },
      "source": [
        "#### Displaying the text after processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JiyRKWFtxwzC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "e020c8ec-8092-4554-c20b-8d4460b60e7e"
      },
      "source": [
        "data['comment_text'][:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        removed speedy tag strength times story alone\n",
              "1    stop continue blank delete portions content te...\n",
              "2    effects   dated  rd june with codicil charlott...\n",
              "3    know personal attacks referring to since blank...\n",
              "4                 expect everyone agree terribly wrong\n",
              "Name: comment_text, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XF9PPXsA2XVe",
        "colab_type": "text"
      },
      "source": [
        "#### Applying stemmer\n",
        "What is steeming ?\n",
        "\n",
        "Stemming is just a simpler version of lemmatization where we are interested in stripping the suffix at the end of the word. When stemming we are interesting in reducing the inflected or derived word to it's base form.\n",
        "\n",
        "What is inflection ?\n",
        "\n",
        "inflections: adding a suffix to a word, that doesn't change its grammatical category, such as tenses in verbs (-ing, -ed, -s), plural in nouns (s).\n",
        "\n",
        "What is derivation ?\n",
        "\n",
        "derivations - adding a suffix to a word, that changes its grammatical category, such as nation (noun) => national (adjective) => nationalize (verb)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9amVOjuecOrr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stemmer = SnowballStemmer(\"english\")\n",
        "def stemming(sentence):\n",
        "    Sentence = \"\"\n",
        "    for word in sentence.split():\n",
        "        stem = stemmer.stem(word)\n",
        "        Sentence += stem\n",
        "        Sentence += \" \"\n",
        "    Sentence = Sentence.strip()\n",
        "    return Sentence\n",
        "data['comment_text'] = data['comment_text'].apply(stemming)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EaRR48E74esO",
        "colab_type": "text"
      },
      "source": [
        "# SPLITTING THE DATA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yT6senTKic0G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train, test = train_test_split(data, random_state=1, test_size=0.2, shuffle=True)\n",
        "tr_text = train['comment_text']\n",
        "te_text = test['comment_text']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJudS25P4mLz",
        "colab_type": "text"
      },
      "source": [
        "# VECTORIZING THE DATA\n",
        "#### TF-IDF Vectorizer\n",
        "\n",
        "There are some words which are insignificant so, we need to remove it. In order to do so, we use TF-IDF,\n",
        "\n",
        "<strong>TF - Term Frequency : </strong>\n",
        "\n",
        "(How many times a term occures in a document)\n",
        "\n",
        "<em><strong>tf(i, j) = n(i, j) / E(n(i, j)</strong></em>\n",
        "\n",
        "<strong>IDF - Inverse Document frequency :</strong>\n",
        "\n",
        "How common a word is across all documents,\n",
        "\n",
        "<em><strong>IDF(w) = log ( N/DF(i) )</em></strong>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceqr4ia5iblH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing library\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Defining the vectorizer\n",
        "vectorizer = TfidfVectorizer(strip_accents='unicode', analyzer='word', ngram_range=(1,3), norm='l2')\n",
        "\n",
        "# Fitting the vectorizer to train and text data\n",
        "vectorizer.fit(tr_text)\n",
        "vectorizer.fit(te_text)\n",
        "\n",
        "# Transforming the training data\n",
        "x_train = vectorizer.transform(tr_text)\n",
        "y_train = train.drop(labels = ['id','comment_text'], axis=1)\n",
        "\n",
        "# Transforming the training data\n",
        "x_test = vectorizer.transform(te_text)\n",
        "y_test = test.drop(labels = ['id','comment_text'], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QUH_SDImv08",
        "colab_type": "text"
      },
      "source": [
        "#### COUNT VECTORIZER\n",
        "CountVectorizer takes the approach of bag of words.\n",
        "\n",
        "This works as follows :\n",
        "\n",
        "<strong>\n",
        "1] Each word inside the document will be separated into tokens.\n",
        "\n",
        "2] Assigning a weight to each token proportional to the frequency with which it shows up in the document and/or corpus.\n",
        "\n",
        "3] Creating a document-term matrix with each row representing a document and each column addressing a token.\n",
        "</strong>\n",
        "\n",
        "<em>Example : doc = ['This is Count vectorizer']</em>\n",
        "\n",
        "1st token = 'This'\n",
        "\n",
        "2nd token = 'is' and so on....\n",
        "\n",
        "Then the number of times each token occures in a document is counted in case of CountVectorizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qV16wGKm7f1E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing library\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Defining the vectorizer\n",
        "cvectorizer = CountVectorizer(strip_accents='unicode', analyzer='word', ngram_range=(1,3))\n",
        "\n",
        "# Fitting the vectorizer to train and text data\n",
        "cvectorizer.fit(tr_text)\n",
        "cvectorizer.fit(te_text)\n",
        "\n",
        "# Transforming the training data\n",
        "cx_train = cvectorizer.transform(tr_text)\n",
        "cy_train = train.drop(labels = ['id','comment_text'], axis=1)\n",
        "\n",
        "# Transforming the training data\n",
        "cx_test = cvectorizer.transform(te_text)\n",
        "cy_test = test.drop(labels = ['id','comment_text'], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7LP2gmh7Gt8",
        "colab_type": "text"
      },
      "source": [
        "# Using scikit-multilearn library for multi-label classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGlmIGkVF2PW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "8e9d536d-eae3-4a5c-db21-07439384bd18"
      },
      "source": [
        "!pip install scikit-multilearn"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting scikit-multilearn\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/1f/e6ff649c72a1cdf2c7a1d31eb21705110ce1c5d3e7e26b2cc300e1637272/scikit_multilearn-0.2.0-py3-none-any.whl (89kB)\n",
            "\r\u001b[K     |███▊                            | 10kB 15.5MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |███████████                     | 30kB 2.1MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 40kB 2.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 51kB 2.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 61kB 2.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 71kB 2.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 81kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 92kB 2.4MB/s \n",
            "\u001b[?25hInstalling collected packages: scikit-multilearn\n",
            "Successfully installed scikit-multilearn-0.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3csxVivjQ-m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from skmultilearn.problem_transform import BinaryRelevance\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from skmultilearn.problem_transform import ClassifierChain\n",
        "from sklearn.linear_model import LogisticRegression"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7NLPfa9scT4y",
        "colab_type": "text"
      },
      "source": [
        "# MODEL\n",
        "#### Applying Tfidf in Binart Relevance(GaussianNB)\n",
        "What is Binary Relevance ?\n",
        "\n",
        "the data is split up into L data sets, where L is the number of labels. Each subset has a column where either a 0 or a 1 is assigned to an instance, indicating the presence or absence of that label on that instance. A separate classifier is trained on each data set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_jUBdj8j6Xe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "9f13ea27-e109-476b-d976-183ab9dc84f8"
      },
      "source": [
        "%%time\n",
        "\n",
        "# using binary relevance\n",
        "# initialize binary relevance multi-label classifier\n",
        "# with a gaussian naive bayes base classifier\n",
        "BR_clf_tfidf = BinaryRelevance(GaussianNB())\n",
        "\n",
        "# train\n",
        "BR_clf_tfidf.fit(x_train, y_train)\n",
        "\n",
        "# predict\n",
        "BR_clf_tfidf_predictions = BR_clf_tfidf.predict(x_test)\n",
        "\n",
        "# accuracy\n",
        "print(f\" Accuracy : {accuracy_score(y_test, BR_clf_tfidf_predictions)*100:.2f} %\")\n",
        "print(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Accuracy : 90.31 %\n",
            "\n",
            "\n",
            "CPU times: user 3.14 s, sys: 414 ms, total: 3.55 s\n",
            "Wall time: 3.56 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtgIxp7jckP3",
        "colab_type": "text"
      },
      "source": [
        "#### Applying Tfidf ClassifierChain(Logistic regression)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7IA9hpLlszS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "975b5720-acbd-45a7-9610-806a513dc4e4"
      },
      "source": [
        "%%time\n",
        "\n",
        "# using classifier chains\n",
        "# initialize classifier chains multi-label classifier\n",
        "CC_clf_tfidf = ClassifierChain(LogisticRegression(penalty = 'l2', C = 0.01))\n",
        "\n",
        "# Training logistic regression model on train data\n",
        "CC_clf_tfidf.fit(x_train, y_train)\n",
        "\n",
        "# predict\n",
        "CC_clf_tfidf_predictions = CC_clf_tfidf.predict(x_test)\n",
        "\n",
        "# accuracy\n",
        "print(f\" Accuracy : {accuracy_score(y_test,CC_clf_tfidf_predictions)*100:.2f} %\")\n",
        "print(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Accuracy : 91.56 %\n",
            "\n",
            "\n",
            "CPU times: user 9.13 s, sys: 813 ms, total: 9.94 s\n",
            "Wall time: 6.31 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lkGqbSQAik6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "5c8f0075-672c-4e96-a97d-6d3a4fd43d1a"
      },
      "source": [
        "# initialize binary relevance multi-label classifier\n",
        "# with a gaussian naive bayes base classifier\n",
        "%%time\n",
        "BR_clf_bow = BinaryRelevance(GaussianNB())\n",
        "\n",
        "# train\n",
        "BR_clf_bow.fit(cx_train, cy_train)\n",
        "\n",
        "# predict\n",
        "BR_clf_bow_predictions = BR_clf_bow.predict(cx_test)\n",
        "\n",
        "# accuracy\n",
        "print(f\" Accuracy : {accuracy_score(cy_test, BR_clf_bow_predictions)*100:.2f} %\")\n",
        "print(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Accuracy : 90.31 %\n",
            "\n",
            "\n",
            "CPU times: user 4.22 s, sys: 33 ms, total: 4.25 s\n",
            "Wall time: 4.26 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvC9QRYrc5qO",
        "colab_type": "text"
      },
      "source": [
        "#### Applying CountVectorizer in ClassifierChain(LogisticRegression)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pkx_PgaJ_l4L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "c5f307ca-2399-438b-a645-da826bbbe0f3"
      },
      "source": [
        "%%time\n",
        "CC_clf_bow = ClassifierChain(LogisticRegression())\n",
        "\n",
        "# Training logistic regression model on train data\n",
        "CC_clf_bow.fit(cx_train, cy_train)\n",
        "\n",
        "# predict\n",
        "CC_clf_bow_predictions = CC_clf_bow.predict(cx_test)\n",
        "\n",
        "# accuracy\n",
        "print(f\" Accuracy : {accuracy_score(cy_test, CC_clf_bow_predictions)*100:.2f} %\")\n",
        "print(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Accuracy : 92.50 %\n",
            "\n",
            "\n",
            "CPU times: user 23.6 s, sys: 1.81 s, total: 25.4 s\n",
            "Wall time: 14.1 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCQTa1b0fPoe",
        "colab_type": "text"
      },
      "source": [
        "#### LOSS\n",
        "‘hamming loss’ value ranges from 0 to 1. As it is a loss metric, its interpretation is reverse in nature unlike normal accuracy ratio. Lesser value of hamming loss indicates a better classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckyfxqOBFw_a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "71277340-947e-4926-8142-4df090775beb"
      },
      "source": [
        "import sklearn.metrics as metrics\n",
        "print(f\" hamming loss for BR-TFIDF: {metrics.hamming_loss(y_test, BR_clf_tfidf_predictions):.2f}\")\n",
        "print(f\" hamming loss for CC-TFIDF: {metrics.hamming_loss(y_test, CC_clf_tfidf_predictions):.2f}\")\n",
        "print(f\" hamming loss for BR-BoW: {metrics.hamming_loss(cy_test, BR_clf_bow_predictions):.2f}\")\n",
        "print(f\" hamming loss for CC-BoW: {metrics.hamming_loss(cy_test, CC_clf_bow_predictions):.2f}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " hamming loss for BR-TFIDF: 0.04\n",
            " hamming loss for CC-TFIDF: 0.04\n",
            " hamming loss for BR-BoW: 0.04\n",
            " hamming loss for CC-BoW: 0.02\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYUhM6rNsir8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "07aba6c3-4008-4be3-c965-7a3bafcb788f"
      },
      "source": [
        "from prettytable import PrettyTable\n",
        "x = PrettyTable()\n",
        "\n",
        "x.field_names = ['Model', 'Vectorizer', 'Accuracy', 'Hamming loss']\n",
        "x.add_row(['BinaryRelevance(GaussianNB)', 'Tfidf', '90.31%', 0.04])\n",
        "x.add_row(['ClassifierChain(LogisticRegression)', 'Tfidf', '91.56%', 0.04])\n",
        "x.add_row(['BinaryRelevance(GaussianNB)', 'BoW', '90.31%', 0.04])\n",
        "x.add_row(['ClassifierChain(LogisticRegression)', 'BoW', '92.50%', 0.02])\n",
        "\n",
        "\n",
        "\n",
        "x.padding_width = 5\n",
        "print(x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------------------------------------------+--------------------+------------------+----------------------+\n",
            "|                    Model                    |     Vectorizer     |     Accuracy     |     Hamming loss     |\n",
            "+---------------------------------------------+--------------------+------------------+----------------------+\n",
            "|         BinaryRelevance(GaussianNB)         |       Tfidf        |      90.31%      |         0.04         |\n",
            "|     ClassifierChain(LogisticRegression)     |       Tfidf        |      91.56%      |         0.04         |\n",
            "|         BinaryRelevance(GaussianNB)         |        BoW         |      90.31%      |         0.04         |\n",
            "|     ClassifierChain(LogisticRegression)     |        BoW         |      92.50%      |         0.02         |\n",
            "+---------------------------------------------+--------------------+------------------+----------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6d5hCuRwifAu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}